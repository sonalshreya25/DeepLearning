{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyOCt18gzdrsG0JW+DptSQ77",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/sonalshreya25/DeepLearning/blob/main/CostandWeight.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "pSA8T0_9Yz9k"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **Assignment 2**\n",
        "# **Exploring Weight Initialization Methods and Cost Functions**\n",
        "\n",
        "\n",
        "\n",
        "Neural networks are a subset of machine learning algorithms inspired by the human brain's structure and function. They consist of interconnected layers of nodes called as neurons that process input data to produce an output. The primary components of a neural network include:\n",
        "\n",
        "**Input Layer**: Receives the input data.\n",
        "\n",
        "**Hidden Layers**: Intermediate layers that transform the input into something the output layer can use.\n",
        "\n",
        "**Output Layer**: Produces the final output.\n",
        "\n",
        "The performance of a neural network is dependent on key factors like:\n",
        "\n",
        "1.   Weight Initialization\n",
        "2.   Cost Function Selection\n",
        "\n",
        "\n",
        "### **Weight Initialization**\n",
        "Weight initialization refers to the process of setting the initial values for the weights of the network before training begins. It plays a crucial role in ensuring stable and efficient training by preventing issues like vanishing or exploding gradients. Proper weight initialization is critical to neural networks to train effectively and converge quickly.\n",
        "\n",
        "Some weight initializing techniques include:\n",
        "\n",
        "\n",
        "\n",
        "1.  **Zero Initialization** : Setting all weights to zero.\n",
        "2.  **Random Initialization** : Assigning small random values to weights.\n",
        "3.  **Kaiming (he) Initialization** :  Specifically designed for layers with ReLU activation functions, it scales the weights based on the number of input units to maintain variance throughout the network\n",
        "4.  **Xavier/Glorot Initialization** : it scales the weights to maintain the variance of activations across layers, it is suitable for layers with sigmoid or tanh activation functions.\n",
        "\n",
        "In this experiment, we would test with He initialization and Glorot Initialization and investigate the impact of different weight initialization strategies on the training performance of a neural network.\n",
        "\n",
        "### **Cost Function Selection**\n",
        "The cost function (or loss function) measures how well the neural network's predictions match the actual data. It guides the optimization process by providing a metric to minimize during training. It determines how well a model learns from the data and influences its convergence behavior\n",
        "\n",
        "Common cost functions include:\n",
        "\n",
        "1.  **Cross-Entropy Loss**: Used for classification tasks, it measures the difference between the predicted probability distribution and the actual distribution.\n",
        "2.   **Mean Squared Error (MSE)**: Used for regression tasks, it calculates the average squared difference between predicted and actual values.\n",
        "3. **Leibler Divergence Loss**: Measures the difference between two probability distributions\n",
        "4. **Huber loss**: A loss function that combines the best properties of mean squared error and mean absolute error, being less sensitive to outliers than MSE and more robust than MAE\n",
        "5. **Log-Cosh Loss**: It is a loss function used in regression tasks that combines the benefits of Mean Squared Error (MSE) and Mean Absolute Error (MAE). It is defined as the logarithm of the hyperbolic cosine of the prediction error, which makes it smooth and less sensitive to outliers.\n",
        "\n",
        "In this experiment, we plan to use Huber Loss and Log-cosh loss functions and analyze the convergence behavior, training stability, and final performance metrics. We thereby, seek to gain insights into the interplay between initialization methods and cost function choices in deep learning.\n",
        "\n",
        "### **Experiment Setup**\n",
        "To evaluate different weight initialization strategies and cost functions, we implemented a simple feedforward neural network for classifying images from the MNIST dataset. The network architecture includes:\n",
        "\n",
        "Input layer: 784 neurons (flattened 28x28 images)\n",
        "\n",
        "Hidden layer: 128 neurons with ReLU activation\n",
        "\n",
        "Output layer: 10 neurons (softmax activation for classification)"
      ],
      "metadata": {
        "id": "Rk6mfEDWY0hY"
      }
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "NRbQZoWOuwAY"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "KjADBwssybyZ"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}
