{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyNU/rVxeLWsPXOCmHXxL2vW",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/sonalshreya25/DeepLearning/blob/main/Assignment2.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "pSA8T0_9Yz9k"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **Exploring Weight Initialization Methods and Cost Functions**\n",
        "\n",
        "\n",
        "\n",
        "Neural networks are a subset of machine learning algorithms inspired by the human brain's structure and function. They consist of interconnected layers of nodes called as neurons that process input data to produce an output. The primary components of a neural network include:\n",
        "\n",
        "**Input Layer**: Receives the input data.\n",
        "\n",
        "**Hidden Layers**: Intermediate layers that transform the input into something the output layer can use.\n",
        "\n",
        "**Output Layer**: Produces the final output.\n",
        "\n",
        "The performance of a neural network is dependent on key factors like:\n",
        "\n",
        "1.   Weight Initialization\n",
        "2.   Cost Function Selection\n",
        "\n",
        "\n",
        "### **Weight Initialization**\n",
        "Weight initialization refers to the process of setting the initial values for the weights of the network before training begins. It plays a crucial role in ensuring stable and efficient training by preventing issues like vanishing or exploding gradients. Proper weight initialization is critical to neural networks to train effectively and converge quickly.\n",
        "\n",
        "Some weight initializing techniques include:\n",
        "\n",
        "\n",
        "\n",
        "1.  **Zero Initialization** : Setting all weights to zero.\n",
        "2.  **Random Initialization** : Assigning small random values to weights.\n",
        "3.  **Kaiming (he) Initialization** :  Specifically designed for layers with ReLU activation functions, it scales the weights based on the number of input units to maintain variance throughout the network\n",
        "4.  **Xavier/Glorot Initialization** : it scales the weights to maintain the variance of activations across layers, it is suitable for layers with sigmoid or tanh activation functions.\n",
        "\n",
        "In this experiment, we would test with He initialization and Glorot Initialization and investigate the impact of different weight initialization strategies on the training performance of a neural network.\n",
        "\n",
        "### **Cost Function Selection**\n",
        "The cost function (or loss function) measures how well the neural network's predictions match the actual data. It guides the optimization process by providing a metric to minimize during training. It determines how well a model learns from the data and influences its convergence behavior\n",
        "\n",
        "Common cost functions include:\n",
        "\n",
        "1.  **Cross-Entropy Loss**: Used for classification tasks, it measures the difference between the predicted probability distribution and the actual distribution.\n",
        "2.   **Mean Squared Error (MSE)**: Used for regression tasks, it calculates the average squared difference between predicted and actual values.\n",
        "3. **Leibler Divergence Loss**: Measures the difference between two probability distributions\n",
        "4. **Huber loss**: A loss function that combines the best properties of mean squared error and mean absolute error, being less sensitive to outliers than MSE and more robust than MAE\n",
        "5. **Log-Cosh Loss**: It is a loss function used in regression tasks that combines the benefits of Mean Squared Error (MSE) and Mean Absolute Error (MAE). It is defined as the logarithm of the hyperbolic cosine of the prediction error, which makes it smooth and less sensitive to outliers.\n",
        "\n",
        "In this experiment, we plan to use Huber Loss and Log-cosh loss functions and analyze the convergence behavior, training stability, and final performance metrics. We thereby, seek to gain insights into the interplay between initialization methods and cost function choices in deep learning.\n",
        "\n",
        "### **Experiment Setup**\n",
        "To evaluate different weight initialization strategies and cost functions, we implemented a simple feedforward neural network for classifying images from the MNIST dataset. The network architecture includes:\n",
        "\n",
        "Input layer: 784 neurons (flattened 28x28 images)\n",
        "\n",
        "Hidden layer: 128 neurons with ReLU activation\n",
        "\n",
        "Output layer: 10 neurons (softmax activation for classification)"
      ],
      "metadata": {
        "id": "Rk6mfEDWY0hY"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Loading data from MNIST dataset and create a dataloader with batchsize 64.\n"
      ],
      "metadata": {
        "id": "ctAPy1K1z1bv"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "import torchvision.transforms as transforms\n",
        "import torchvision.datasets as datasets\n",
        "transform = transforms.Compose([transforms.ToTensor(), transforms.Normalize((0.5,), (0.5,))])\n",
        "trainset = datasets.MNIST(root='./data', train=True, download=True, transform=transform)\n",
        "trainloader = torch.utils.data.DataLoader(trainset, batch_size=64, shuffle=True)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "NRbQZoWOuwAY",
        "outputId": "814529c3-9f9b-4952-89ff-e0ddb52041e0"
      },
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Downloading http://yann.lecun.com/exdb/mnist/train-images-idx3-ubyte.gz\n",
            "Failed to download (trying next):\n",
            "HTTP Error 404: Not Found\n",
            "\n",
            "Downloading https://ossci-datasets.s3.amazonaws.com/mnist/train-images-idx3-ubyte.gz\n",
            "Downloading https://ossci-datasets.s3.amazonaws.com/mnist/train-images-idx3-ubyte.gz to ./data/MNIST/raw/train-images-idx3-ubyte.gz\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 9.91M/9.91M [00:00<00:00, 12.8MB/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Extracting ./data/MNIST/raw/train-images-idx3-ubyte.gz to ./data/MNIST/raw\n",
            "\n",
            "Downloading http://yann.lecun.com/exdb/mnist/train-labels-idx1-ubyte.gz\n",
            "Failed to download (trying next):\n",
            "HTTP Error 404: Not Found\n",
            "\n",
            "Downloading https://ossci-datasets.s3.amazonaws.com/mnist/train-labels-idx1-ubyte.gz\n",
            "Downloading https://ossci-datasets.s3.amazonaws.com/mnist/train-labels-idx1-ubyte.gz to ./data/MNIST/raw/train-labels-idx1-ubyte.gz\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 28.9k/28.9k [00:00<00:00, 341kB/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Extracting ./data/MNIST/raw/train-labels-idx1-ubyte.gz to ./data/MNIST/raw\n",
            "\n",
            "Downloading http://yann.lecun.com/exdb/mnist/t10k-images-idx3-ubyte.gz\n",
            "Failed to download (trying next):\n",
            "HTTP Error 404: Not Found\n",
            "\n",
            "Downloading https://ossci-datasets.s3.amazonaws.com/mnist/t10k-images-idx3-ubyte.gz\n",
            "Downloading https://ossci-datasets.s3.amazonaws.com/mnist/t10k-images-idx3-ubyte.gz to ./data/MNIST/raw/t10k-images-idx3-ubyte.gz\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 1.65M/1.65M [00:00<00:00, 3.17MB/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Extracting ./data/MNIST/raw/t10k-images-idx3-ubyte.gz to ./data/MNIST/raw\n",
            "\n",
            "Downloading http://yann.lecun.com/exdb/mnist/t10k-labels-idx1-ubyte.gz\n",
            "Failed to download (trying next):\n",
            "HTTP Error 404: Not Found\n",
            "\n",
            "Downloading https://ossci-datasets.s3.amazonaws.com/mnist/t10k-labels-idx1-ubyte.gz\n",
            "Downloading https://ossci-datasets.s3.amazonaws.com/mnist/t10k-labels-idx1-ubyte.gz to ./data/MNIST/raw/t10k-labels-idx1-ubyte.gz\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 4.54k/4.54k [00:00<00:00, 3.76MB/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Extracting ./data/MNIST/raw/t10k-labels-idx1-ubyte.gz to ./data/MNIST/raw\n",
            "\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Defining the Weight Initialization Function\n",
        "\n",
        "*    He initialization is a weight initialization technique designed for ReLU\n",
        "\n",
        "activation functions.It sets the weights to be drawn from a normal distribution with a mean of 0 and a variance of $\\frac{2}{n_{\\text{in}}}$\n",
        " , where $n_{\\text{in}}$ is the number of input units to the neuron.\n",
        "$$\n",
        "W \\sim \\mathcal{N}\\left( 0, \\frac{2}{n_{\\text{in}}} \\right)\n",
        "$$\n",
        "\n",
        "* Xavier initialization (also known as Glorot initialization) is designed for activation functions like sigmoid or tanh, where the weights are drawn from a normal distribution with a mean of 0 and variance $\\frac{1}{n_{\\text{in}}}$ , where $n_{\\text{in}}$   is the number of input units to the neuron.\n",
        "\n",
        "$$\n",
        "W \\sim \\mathcal{N}\\left( 0, \\frac{1}{n_{\\text{in}}} \\right)\n",
        "$$\n"
      ],
      "metadata": {
        "id": "RPmmQo3i0Kg4"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def initialize_weights(model, method=\"xavier\"):\n",
        "    for m in model.modules():\n",
        "        if isinstance(m, nn.Linear):\n",
        "            if method == \"xavier\":\n",
        "                nn.init.xavier_uniform_(m.weight)\n",
        "            elif method == \"he\":\n",
        "                nn.init.kaiming_uniform_(m.weight, nonlinearity='relu')\n"
      ],
      "metadata": {
        "id": "5cfmJbe40SVU"
      },
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [],
      "metadata": {
        "id": "X2xcYYd60_S2"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Defining the neural network\n",
        "class NeuralNet(nn.Module):\n",
        "    def __init__(self):\n",
        "        super(NeuralNet, self).__init__()\n",
        "        # Input layer (784 to 128 neurons)\n",
        "        self.fc1 = nn.Linear(28*28, 128)\n",
        "        # Activation function\n",
        "        self.relu = nn.ReLU()\n",
        "        # Output layer (128 to 10 classes)\n",
        "        self.fc2 = nn.Linear(128, 10)\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = x.view(-1, 28*28)  # Flatten input image\n",
        "        x = self.fc1(x)\n",
        "        x = self.relu(x)\n",
        "        x = self.fc2(x)\n",
        "        return x"
      ],
      "metadata": {
        "id": "Ag-V4plm1CSH"
      },
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "The Cost functions we will be using for this experiment are\n",
        "\n",
        "\n",
        "*   **Huber Loss** : Huber loss is a combination of MSE and absolute error, which is less sensitive to outliers than MSE. The formula for Huber loss is\n",
        "\n",
        "$$\n",
        "L_{\\delta}(y, \\hat{y}) =\n",
        "\\begin{cases}\n",
        "\\frac{1}{2}(y - \\hat{y})^2 & \\text{if } |y - \\hat{y}| \\leq \\delta \\\\\n",
        "\\delta |y - \\hat{y}| - \\frac{1}{2} \\delta^2 & \\text{otherwise}\n",
        "\\end{cases}\n",
        "$$\n",
        "Where:\n",
        "\n",
        "- $y$ is the true value.\n",
        "- $\\hat{y}$ is the predicted value.\n",
        "- $\\delta$ is a hyperparameter that controls the threshold between quadratic and linear loss.\n",
        "\n",
        "**Advantages** :\n",
        "\n",
        "*   Robust to outliers: Less sensitive to large errors compared to MSE\n",
        "*   Smooth gradient: Provides a balance between MSE and absolute error, aiding faster convergence\n",
        "\n",
        "*   **Log-Cosh Loss** : Log-Cosh loss is a smoother version of MSE that is less sensitive to large errors. The formula for Log-Cosh loss is:\n",
        "\n",
        "$$\n",
        "L_{\\text{log-cosh}}(y, \\hat{y}) = \\frac{1}{n} \\sum_{i=1}^{n} \\text{log}\\left( \\cosh(y_i - \\hat{y}_i) \\right)\n",
        "$$\n",
        "Where:\n",
        "\n",
        "- $\\cosh(x) = \\frac{e^x + e^{-x}}{2}$ is the hyperbolic cosine function.\n",
        "- $y_i$ and $\\hat{y}_i$ are the true and predicted values for the $i$-th data point.\n",
        "\n",
        "**Advantages**:\n",
        "\n",
        "\n",
        "*   Less sensitive to large errors: Penalizes large errors less than MSE\n",
        "*   Smoother convergence: Leads to more stable training with a smoother gradient.\n",
        "\n",
        "Huber loss and Log-Cosh loss offer more robust alternatives to Mean Squared Error, especially when handling noisy data or outliers.\n",
        "\n"
      ],
      "metadata": {
        "id": "-F3eRxaV8rk2"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Implementing Cost Function\n",
        "def huber_loss(output, target, delta=1.0):\n",
        "    error = target - output\n",
        "    loss = torch.where(torch.abs(error) < delta, 0.5 * error**2, delta * (torch.abs(error) - 0.5 * delta))\n",
        "    return loss.mean()\n",
        "\n",
        "def log_cosh_loss(output, target):\n",
        "    return torch.mean(torch.log(torch.cosh(output - target)))"
      ],
      "metadata": {
        "id": "Ipy_TwMC1dxR"
      },
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Training loop\n",
        "def train_model(init_method, loss_fn):\n",
        "    model = NeuralNet()\n",
        "    initialize_weights(model, method=init_method)\n",
        "    optimizer = optim.Adam(model.parameters(), lr=0.001)\n",
        "    loss_function = huber_loss if loss_fn == \"huber\" else log_cosh_loss\n",
        "\n",
        "    for epoch in range(5):\n",
        "        for images, labels in trainloader:\n",
        "            optimizer.zero_grad()\n",
        "            outputs = model(images)\n",
        "            labels_one_hot = torch.nn.functional.one_hot(labels, num_classes=10).float()\n",
        "            loss = loss_function(outputs, labels_one_hot)\n",
        "            loss.backward()\n",
        "            optimizer.step()\n",
        "    print(f\"Final loss with {init_method} and {loss_fn}: {loss.item()}\")\n",
        "\n"
      ],
      "metadata": {
        "id": "KDQ5e2ZG1u6g"
      },
      "execution_count": 12,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "train_model(\"he\", \"huber\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "PmiMvsdy29MD",
        "outputId": "331387b4-2e0f-4bf6-a285-ff20c5010d1b"
      },
      "execution_count": 13,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Final loss with he and huber: 0.005685228854417801\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "train_model(\"xavier\", \"huber\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "GYoBx6UR3ugE",
        "outputId": "b21d4ee3-c533-41f9-e71b-1d3705dae7b4"
      },
      "execution_count": 14,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Final loss with xavier and huber: 0.0058444500900805\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "train_model(\"xavier\", \"log_cosh\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "eJD7ETeO4NRk",
        "outputId": "d48cc761-b920-4adf-e022-cf8224db3ddb"
      },
      "execution_count": 15,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Final loss with xavier and log_cosh: 0.008070656098425388\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "train_model(\"he\", \"log_cosh\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "YOkGqLtu4yAV",
        "outputId": "63672fba-ca01-4625-d5a1-e6324b5744df"
      },
      "execution_count": 16,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Final loss with he and log_cosh: 0.005941941402852535\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## **Conclusion**\n",
        "In this experiment, we tested a neural network using two weight initialization methods—He and Xavier—along with two cost functions: Huber loss and Log-Cosh loss.\n",
        "\n",
        "The results showed that He initialization with Huber loss gave the best performance, with the lowest final loss (0.0057). This suggests that He initialization works well for ReLU activations, and Huber loss handles errors effectively without being too sensitive to outliers. On the other hand, Xavier initialization with Log-Cosh loss had the highest final loss (0.0081), making it less effective in this case.\n",
        "\n",
        "In conclusion, He initialization with Huber loss is the best combination for this neural network, as it leads to the lowest loss.\n",
        "Kaimen initialization performed best because it's specifically designed for ReLU activation functions. ReLU is known for causing problems like \"vanishing gradients\" or \"exploding gradients\" in deep networks, especially with standard initialization methods. He initialization addresses this by scaling the weights to account for the number of inputs, which helps maintain stable gradients during backpropagation and allows the network to learn effectively.\n",
        "\n",
        "On the other-hand, Huber loss also contributed to the better performance because it combines the strengths of mean squared error (MSE) and absolute error. It is less sensitive to outliers than MSE. This makes it effective in situations where one would want to balance error sensitivity and robustness, leading to a lower final loss"
      ],
      "metadata": {
        "id": "ogm70dvr56qG"
      }
    }
  ]
}
